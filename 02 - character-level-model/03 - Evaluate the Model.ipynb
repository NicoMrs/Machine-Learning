{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea2e053f-cfdb-487a-9f58-1c4f04f6a67c",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d6da745-544f-48e4-be5f-b38da6ddfc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643f6cf3-53c8-4867-9990-2866af9d1db9",
   "metadata": {},
   "source": [
    "## I. Set up the model from saved results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fba6d2-d518-4318-b0cc-e3468747ffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('openpyxl.txt', 'r', encoding=\"utf8\") as fp:\n",
    "    text=fp.read()\n",
    "\n",
    "char_set = set(text) # make a set to count the unique characters\n",
    "chars_sorted = sorted(char_set)\n",
    "char2int = {ch:i for i,ch in enumerate(chars_sorted)} # contains our mapping\n",
    "char_array = np.array(chars_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d94cd12a-6e65-4d63-b1f0-649ac42c6031",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9f91a5f-8e0d-4691-9d37-fd25e0ffcb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim) \n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, \n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embedding(x).unsqueeze(1)\n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "        out = self.fc(out).reshape(out.size(0), -1)\n",
    "        return out, hidden, cell\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        return hidden.to(device), cell.to(device)\n",
    "    \n",
    "vocab_size = len(char_array)\n",
    "embed_dim = 256\n",
    "rnn_hidden_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49af3189-3cb5-471f-9dac-0932b6034f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(97, 256)\n",
       "  (rnn): LSTM(256, 512, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=97, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n",
    "model.load_state_dict(torch.load('model.pth', weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2d7787-377d-4a56-9bfc-c21aa14f848e",
   "metadata": {},
   "source": [
    "## II. Evaluation Function\n",
    "\n",
    "Once initialized with the the input string, we can start evaluating the sequence.  \n",
    "\n",
    "\n",
    "Each new character is evaluated from previous ones. Predicted logits are associated to each word of the vocab. To ensure, we don't get the same answer everytime, we sample the predicted distribution from the logits. We introduce a ```scale_factor``` which represent the temperature parameter and control the randomness. \n",
    "\n",
    "If ```scale_factor > 1.0``` the model is more certain, if ```scale_factor < 1.0``` we add more randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "bc024636-259a-452d-9284-28b350428087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use for sampling\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5d40493a-ffb9-4c4a-af80-7de086f66660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, starting_str, len_generated_text, scale_factor):\n",
    "\n",
    "    # 1. Intialize model with input sequence\n",
    "    encoded_input = torch.tensor([char2int[s] for s in starting_str])\n",
    "    encoded_input = torch.reshape(encoded_input, (1, -1))\n",
    "    \n",
    "    generated_str = starting_str\n",
    "    \n",
    "    model.eval()\n",
    "    hidden, cell = model.init_hidden(1)\n",
    "    for c in range(len(starting_str)-1):\n",
    "        _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell) \n",
    "    \n",
    "    # 2. Generate sequence successively\n",
    "    last_char = encoded_input[:, -1]\n",
    "    for i in range(len_generated_text):\n",
    "        logits, hidden, cell = model(last_char.view(1), hidden, cell) \n",
    "        logits = torch.squeeze(logits, 0)\n",
    "        scaled_logits = logits * scale_factor # temperature variable to control randomness\n",
    "        m = Categorical(logits=scaled_logits)\n",
    "        last_char = m.sample()\n",
    "        generated_str += str(char_array[last_char])\n",
    "\n",
    "    return generated_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1b070714-ff11-47d9-b00f-5405c5174cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default = blurRade\n",
      "        self.showVertical = grouping\n",
      "        self.showVertical = grouping\n",
      "        self.showVertical = grouping\n",
      "        self.showVertical = grouping\n",
      "        self.border = value\n",
      "\n",
      "\n",
      "class ConditionalFormat(Serialisable):\n",
      "\n",
      "    tagname = \"pivotCache\"\n",
      "\n",
      "    t = Set(values=(['self.conditionalFormats-vell')\n",
      "\n",
      "    def __init__(self,\n",
      "                 showVerts=None,\n",
      "                 showVerts=None,\n",
      "                 showVerts=None,\n",
      "                 stdDevSubtotal=None,\n",
      "                 showVer\n"
     ]
    }
   ],
   "source": [
    "print(sample(model, starting_str='def', len_generated_text=500, scale_factor=10.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "be938d1e-b349-4690-802c-16c47bd2ef70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ColorDescriptor(Serialisable):\n",
      "\n",
      "    tagname = \"pivotCache\"\n",
      "\n",
      "    txPr = Typed(expected_type=Series, allow_none=True)\n",
      "    showVerts = Bool(allow_none=True)\n",
      "    extLst = Typed(expected_type=Series, allow_none=True)\n",
      "    showVerts = Bool(allow_none=True)\n",
      "    showVerts = Bool(allow_none=True)\n",
      "    showVerts = Bool(allow_none=True)\n",
      "    showVerts = Bool(allow_none=True)\n",
      "    showVerts = Bool(allow_none=True)\n",
      "    showValue = Bool(allow_none=True)\n",
      "    showVerts = Bool(allow_none=True)\n",
      "    showVerts = Bool(\n"
     ]
    }
   ],
   "source": [
    "print(sample(model, starting_str='class', len_generated_text=500, scale_factor=10.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f60e84d0-5c3f-4b50-85f3-ba20b15f5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@property\n",
      "    def authors = None\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n"
     ]
    }
   ],
   "source": [
    "print(sample(model, starting_str='@', len_generated_text=500, scale_factor=5.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f690614-b7af-41cf-bb3c-6b9df57b5f8c",
   "metadata": {},
   "source": [
    "We are able to generate some text that ressemble proper code. Althoug we are far away from proper code, it is still remarkable what a simple model can achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c925bb2-1508-4b40-920d-76e1a1734371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
