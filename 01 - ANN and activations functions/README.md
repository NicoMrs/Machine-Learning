# Showcase the importance of activation functions in Neural Network

This simple notebooks aims at showing the importance of activation functions in neural networks. We will solve the XOR problem 
which is a non linear classification problem and show that linear can only achieve linear. We will try several activations 
functions: **Linear**, **Tanh**, **ReLU**, **Sigmo√Ød** with deep and shallow networks.